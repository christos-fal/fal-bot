from typing import Literal

import discord
from discord import app_commands
import fal_client

from fal_bot import config, utils
from fal_bot.consts import OVI_RESOLUTIONS

# Configure fal_client with your API key
fal_client.api_key = config.FAL_SECRET

@app_commands.command(
    name="ovi",
    description="Generate a video using Ovi text-to-video or image-to-video model",
)
@app_commands.autocomplete(resolution=utils.autocomplete_from(OVI_RESOLUTIONS))
async def command(
    interaction: discord.Interaction,
    prompt: str,
    mode: Literal["text-to-video", "image-to-video"] = "text-to-video",
    negative_prompt: str = "jitter, bad hands, blur, distortion",
    num_inference_steps: int = 30,
    audio_negative_prompt: str = "robotic, muffled, echo, distorted",
    resolution: str = "992x512",
    seed: int | None = None,
    image: discord.Attachment | None = None,
):
    # Validate image requirement for image-to-video mode
    if mode == "image-to-video" and image is None:
        await interaction.response.send_message(
            "❌ Image is required for image-to-video mode. Please upload an image.",
            ephemeral=True
        )
        return
    
    # Validate image type if provided
    if image and not image.content_type.startswith('image/'):
        await interaction.response.send_message(
            "❌ Please upload a valid image file.",
            ephemeral=True
        )
        return

    await interaction.response.send_message("Your video generation request has been received.")
    
    # Determine the model endpoint and arguments based on mode
    if mode == "text-to-video":
        model_endpoint = "fal-ai/ovi"
        arguments = {
            "prompt": prompt,
            "negative_prompt": negative_prompt,
            "num_inference_steps": num_inference_steps,
            "audio_negative_prompt": audio_negative_prompt,
            "resolution": resolution,
        }
    else:  # image-to-video
        model_endpoint = "fal-ai/ovi/image-to-video"
        arguments = {
            "prompt": prompt,
            "negative_prompt": negative_prompt,
            "num_inference_steps": num_inference_steps,
            "audio_negative_prompt": audio_negative_prompt,
            "image_url": image.url,
        }
    
    if seed is not None:
        arguments["seed"] = seed
    
    # Track time
    with utils.Timed() as timer:
        try:
            # Define callback for queue updates
            def on_queue_update(update):
                # This runs synchronously, so we can't await Discord updates here
                if isinstance(update, fal_client.InProgress):
                    for log in update.logs:
                        print(f"[Ovi] {log.get('message', '')}")
            
            # Submit request and wait for result
            result = await fal_client.subscribe_async(
                model_endpoint,
                arguments=arguments,
                with_logs=True,
                on_queue_update=on_queue_update,
            )
            
        except Exception as e:
            await interaction.edit_original_response(
                content=f"❌ Error generating video: {str(e)}"
            )
            return

    if result is None:
        await interaction.edit_original_response(
            content="❌ Failed to generate video. Please try again."
        )
        return

    # Create video embed
    embed = utils.make_video_embed(
        title="Ovi Generated Video",
        video_url=result["video"]["url"],
        prompt=prompt,
        fields={
            "Mode": mode,
            "Resolution": resolution if mode == "text-to-video" else "N/A",
            "Inference Steps": num_inference_steps,
            "Seed": result.get("seed", "Random"),
            "Time Taken": f"{timer.elapsed:.2f}s",
            "Generated by": interaction.user.mention,
        },
    )

    await interaction.edit_original_response(
        content=None,
        embed=embed,
    )